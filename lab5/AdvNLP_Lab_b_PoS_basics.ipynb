{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_WBq3DbTfUs"
      },
      "source": [
        "# AdvNLP Lab B: using PoS taggers\n",
        "\n",
        "## Session goal\n",
        "\n",
        "The goal of this session is to help you familiarize with PoS tagging. We'll be using NLTK, Stanza, and Spacy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cel-uA9PTfUu",
        "outputId": "f3663326-8612-43fa-c22d-0a769a412376",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "     --------- ------------------------------ 2.9/12.8 MB 27.9 MB/s eta 0:00:01\n",
            "     --------------------------- ------------ 8.7/12.8 MB 29.9 MB/s eta 0:00:01\n",
            "     --------------------------------------- 12.8/12.8 MB 32.1 MB/s eta 0:00:00\n",
            "Installing collected packages: en-core-web-sm\n",
            "Successfully installed en-core-web-sm-3.8.0\n",
            "✔ Download and installation successful\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 27.1MB/s]                    \n",
            "2025-03-20 14:53:33 INFO: Downloaded file to C:\\Users\\andre\\stanza_resources\\resources.json\n",
            "2025-03-20 14:53:33 INFO: Downloading default packages for language: en (English) ...\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.10.0/models/default.zip: 100%|██████████| 526M/526M [00:10<00:00, 48.4MB/s] \n",
            "2025-03-20 14:53:45 INFO: Downloaded file to C:\\Users\\andre\\stanza_resources\\en\\default.zip\n",
            "2025-03-20 14:53:49 INFO: Finished downloading models and saved to C:\\Users\\andre\\stanza_resources\n",
            "2025-03-20 14:53:49 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, ?B/s]                        \n",
            "2025-03-20 14:53:49 INFO: Downloaded file to C:\\Users\\andre\\stanza_resources\\resources.json\n",
            "2025-03-20 14:53:49 WARNING: Language en package default expects mwt, which has been added\n",
            "2025-03-20 14:53:49 INFO: Loading these models for language: en (English):\n",
            "=================================\n",
            "| Processor | Package           |\n",
            "---------------------------------\n",
            "| tokenize  | combined          |\n",
            "| mwt       | combined          |\n",
            "| pos       | combined_charlm   |\n",
            "| lemma     | combined_nocharlm |\n",
            "=================================\n",
            "\n",
            "2025-03-20 14:53:49 INFO: Using device: cpu\n",
            "2025-03-20 14:53:49 INFO: Loading: tokenize\n",
            "2025-03-20 14:53:52 INFO: Loading: mwt\n",
            "2025-03-20 14:53:52 INFO: Loading: pos\n",
            "2025-03-20 14:53:54 INFO: Loading: lemma\n",
            "2025-03-20 14:53:55 INFO: Done loading processors!\n"
          ]
        }
      ],
      "source": [
        "#! pip install stanza\n",
        "import stanza\n",
        "\n",
        "\n",
        "from nltk.tag import PerceptronTagger\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from nltk import download\n",
        "download('averaged_perceptron_tagger')\n",
        "download('punkt')\n",
        "\n",
        "import spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "text='I really like this class. This lab is going to be fun.'\n",
        "spacy_analyzer = spacy.load('en_core_web_sm')\n",
        "\n",
        "stanza.download('en')\n",
        "stanza_pipeline = stanza.Pipeline(lang='en', processors='tokenize,pos,lemma')\n",
        "\n",
        "\n",
        "def run_stanza(text):\n",
        "\n",
        "    pairs=[]\n",
        "    doc = stanza_pipeline(text)\n",
        "    for sent in doc.sentences:\n",
        "        for word in sent.words:\n",
        "            pairs.append((word.text, word.xpos))\n",
        "    return pairs\n",
        "\n",
        "def run_spacy(text):\n",
        "\n",
        "    doc = spacy_analyzer(text)\n",
        "    return [(token, token.tag_) for token in doc]\n",
        "\n",
        "def run_nltk (text):\n",
        "\n",
        "    tagger = PerceptronTagger()\n",
        "    return tagger.tag(word_tokenize(text))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XGViCBUWTfUv"
      },
      "outputs": [],
      "source": [
        "def visualize_pos_results (text):\n",
        "\n",
        "    stanza_pairs = run_stanza(text)\n",
        "    spacy_pairs = run_spacy(text)\n",
        "    nltk_pairs = run_nltk(text)\n",
        "\n",
        "    if len(stanza_pairs)==len(spacy_pairs)==len(nltk_pairs):\n",
        "        tokens = [x[0] for x in stanza_pairs]\n",
        "        stanza_tags = [x[1] for x in stanza_pairs]\n",
        "        spacy_tags = [x[1] for x in spacy_pairs]\n",
        "        nltk_tags = [x[1] for x in nltk_pairs]\n",
        "\n",
        "        import pandas as pd\n",
        "        df=pd.DataFrame(columns = ['tokens','Stanza', 'NLTK', 'Spacy'])\n",
        "        df['tokens']=tokens\n",
        "        df['Stanza']=stanza_tags\n",
        "        df['NLTK']=nltk_tags\n",
        "        df['Spacy']=spacy_tags\n",
        "\n",
        "        print (df)\n",
        "\n",
        "    else:\n",
        "        print ('-'*30)\n",
        "        print ('Stanza')\n",
        "        print (stanza_pairs)\n",
        "        print ('-'*30)\n",
        "        print ('NLTK')\n",
        "        print (nltk_pairs)\n",
        "        print ('-'*30)\n",
        "        print ('Spacy')\n",
        "        print (spacy_pairs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xBxPphKaTfUx"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
            "[nltk_data] Downloading package tagsets to\n",
            "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping help\\tagsets.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n",
            "c:\\Users\\andre\\Documents\\SUPSI\\NLP\\labs\\.advNLP\\Lib\\site-packages\\nltk\\app\\__init__.py:45: UserWarning: nltk.app.wordfreq not loaded (requires the matplotlib library).\n",
            "  warnings.warn(\"nltk.app.wordfreq not loaded (requires the matplotlib library).\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'preposition or conjunction, subordinating'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.data import load\n",
        "from nltk import download\n",
        "download('punkt_tab')\n",
        "download('tagsets')\n",
        "download('averaged_perceptron_tagger_eng')\n",
        "tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
        "tagdict['IN'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hKSJ-fdATfUy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "           tokens Stanza NLTK Spacy\n",
            "0         Traffic     NN   JJ    NN\n",
            "1      congestion     NN   NN    NN\n",
            "2              in     IN   IN    IN\n",
            "3             the     DT   DT    DT\n",
            "4           Shire    NNP  NNP   NNP\n",
            "5              is    VBZ  VBZ   VBZ\n",
            "6         getting    VBG  VBG   VBG\n",
            "7           worse    JJR  JJR   JJR\n",
            "8               .      .    .     .\n",
            "9           After     IN   IN    IN\n",
            "10             we    PRP  PRP   PRP\n",
            "11         landed    VBD  VBD   VBD\n",
            "12             at     IN   IN    IN\n",
            "13        Baggins    NNP  NNP   NNP\n",
            "14  international     JJ   JJ   NNP\n",
            "15        airport     NN   NN    NN\n",
            "16              ,      ,    ,     ,\n",
            "17             we    PRP  PRP   PRP\n",
            "18            got    VBD  VBD   VBD\n",
            "19          stuck     JJ  VBN   VBN\n",
            "20             on     IN   IN    IN\n",
            "21            the     DT   DT    DT\n",
            "22           ring     NN   NN    NN\n",
            "23           road     NN   NN    NN\n",
            "24         around     IN   IN    IN\n",
            "25       Hobbiton    NNP  NNP   NNP\n",
            "26              .      .    .     .\n"
          ]
        }
      ],
      "source": [
        "sentence = \"Traffic congestion in the Shire is getting worse. After we landed at Baggins international airport, we got stuck on the ring road around Hobbiton.\"\n",
        "visualize_pos_results(sentence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "r2CDq4LATfUz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  tokens Stanza NLTK Spacy\n",
            "0   Back     IN   RB    IN\n",
            "1     me    PRP  PRP   PRP\n",
            "2     up     RP   RP    RP\n",
            "3      .      .    .     .\n",
            "  tokens Stanza NLTK Spacy\n",
            "0      I    PRP  PRP   PRP\n",
            "1  asked    VBD  VBD   VBD\n",
            "2   them    PRP  PRP   PRP\n",
            "3     to     TO   TO    TO\n",
            "4   back     VB   VB    VB\n",
            "5     me    PRP  PRP   PRP\n",
            "6     up     RP   RP    RP\n",
            "7      .      .    .     .\n"
          ]
        }
      ],
      "source": [
        "sentence_1='Back me up.'\n",
        "visualize_pos_results(sentence_1)\n",
        "\n",
        "sentence_2='I asked them to back me up.'\n",
        "visualize_pos_results(sentence_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pJFIN9yTfU0"
      },
      "source": [
        "**When** can have many multiple PoS tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8D1NSrq3TfU0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  tokens Stanza NLTK Spacy\n",
            "0   When    WRB  WRB   WRB\n",
            "1    did    VBD  VBD   VBD\n",
            "2    you    PRP  PRP   PRP\n",
            "3   last     RB   JJ    RB\n",
            "4     go     VB   VB    VB\n",
            "5     to     IN   TO    IN\n",
            "6   Bern    NNP  NNP   NNP\n",
            "7      ?      .    .     .\n",
            "     tokens Stanza  NLTK Spacy\n",
            "0     Raise     VB    VB    VB\n",
            "1      your   PRP$  PRP$  PRP$\n",
            "2      hand     NN    NN    NN\n",
            "3      when    WRB   WRB   WRB\n",
            "4       you    PRP   PRP   PRP\n",
            "5       're    VBP   VBP   VBP\n",
            "6  finished    VBN   VBN   VBN\n"
          ]
        }
      ],
      "source": [
        "sentences=['When did you last go to Bern?',   # interrogative adverb\n",
        "'Raise your hand when you\\'re finished']  # conjunction\n",
        "\n",
        "for sentence in sentences:\n",
        "    dflist = visualize_pos_results(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XQBbYWmrTfU1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Wh-adverb'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tagdict['WRB'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QvAbx7n-TfU2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'pronoun, possessive'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tagdict['PRP$'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLQTQjNCTfU2"
      },
      "source": [
        "What's happening in the following example? Which PoS tagger does better?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "riCcvwTCTfU4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        tokens Stanza NLTK Spacy\n",
            "0           An     DT   DT    DT\n",
            "1  experienced     JJ   JJ    JJ\n",
            "2          man     NN   NN    NN\n",
            "3       should     MD   MD    MD\n",
            "4       always     RB   RB    RB\n",
            "5          man     VB   NN    VB\n",
            "6          the     DT   DT    DT\n",
            "7         ship     NN   NN    NN\n",
            "   tokens Stanza NLTK Spacy\n",
            "0   Never     RB   RB    RB\n",
            "1     has    VBZ  VBZ   VBZ\n",
            "2      so     RB   RB    RB\n",
            "3    much     RB   JJ    JJ\n",
            "4    been    VBN  VBN   VBN\n",
            "5    owed    VBN  VBN   VBN\n",
            "6      to     IN   TO    IN\n",
            "7      so     RB   RB    RB\n",
            "8    many     JJ   JJ    JJ\n",
            "9      by     IN   IN    IN\n",
            "10     so     RB   IN    RB\n",
            "11    few     JJ   JJ    JJ\n",
            "          tokens Stanza NLTK Spacy\n",
            "0              A     DT   DT    DT\n",
            "1         nation     NN   NN    NN\n",
            "2           will     MD   MD    MD\n",
            "3            not     RB   RB    RB\n",
            "4        survive     VB   VB    VB\n",
            "5        morally     RB   RB    RB\n",
            "6             or     CC   CC    CC\n",
            "7   economically     RB   RB    RB\n",
            "8           when    WRB  WRB   WRB\n",
            "9             so     RB   RB    RB\n",
            "10           few     JJ   JJ    JJ\n",
            "11          have    VBP  VBP   VBP\n",
            "12            so     RB   RB    RB\n",
            "13          much     JJ   JJ    JJ\n",
            "14           and     CC   CC    CC\n",
            "15            so     RB   RB    RB\n",
            "16          many     JJ   JJ    JJ\n",
            "17          have    VBP  VBP   VBP\n",
            "18            so     RB   RB    RB\n",
            "19        little     JJ   JJ    JJ\n"
          ]
        }
      ],
      "source": [
        "sentences=['An experienced man should always man the ship',\n",
        "'Never has so much been owed to so many by so few',\n",
        "           'A nation will not survive morally or economically \\\n",
        "when so few have so much and so many have so little']\n",
        "\n",
        "for sentence in sentences:\n",
        "    dflist = visualize_pos_results(sentence)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
