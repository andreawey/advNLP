{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Byte Pair Encoding\n",
    "\n",
    "Implement Byte Pair Encoding (BPE) from scratch using basic Python functionalities. <br>\n",
    "Test the correctness of your implementation by reproducing the example in chapter 2.5.2 of \"Speech and Language Processing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the book: Jurafsky, Daniel, and James H. Martin. *Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition with Language Models*, 3rd edition. Online manuscript released January 12, 2025. [Available here](https://web.stanford.edu/~jurafsky/slp3).\n",
    "\n",
    "a pseudocode representing the BPE algorithm is available:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## insert image here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def tokenizeCorpus(corpus: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Function that tokenizes a corpus into individual characters, \n",
    "    then white-space-separates them to give a set of strings\n",
    "    and adds a special end-of-word symbol.\n",
    "\n",
    "    @param str corpus: Input corpus that should be split\n",
    "    @return: List of tokenized words\n",
    "    \"\"\"\n",
    "    # split corpus into words\n",
    "    words = corpus.split(' ')\n",
    "\n",
    "    # Add special end-of-word character '_'\n",
    "    tokenizedCorpus = []\n",
    "    for word in words:\n",
    "        tokenizedCorpus.append(' '.join(list(word)) + ' _')\n",
    "\n",
    "    return tokenizedCorpus\n",
    "\n",
    "def MostFreqPair(C: list[str]) -> tuple[str]:\n",
    "    \"\"\"\n",
    "    Returns the most frequent pair of adjacent tokens in C\n",
    "\n",
    "    @param list[str] C: A list of tokenized words\n",
    "    @return: The most frequent adjacent token pair (as a tuple)\n",
    "    \"\"\"\n",
    "    pairs = {}\n",
    "    # Count pairs of adjacent tokens\n",
    "    for word in C:\n",
    "        tokens = word.split()  # Split the word into tokens\n",
    "        for i in range(len(tokens)-1):\n",
    "            pair = (tokens[i], tokens[i+1])  # adjacent pair\n",
    "            if pair in pairs:\n",
    "                pairs[pair] += 1  # Increment frequency count\n",
    "            else:\n",
    "                pairs[pair] = 1  # Add new pair with count 1\n",
    "\n",
    "    # Return the most frequent pair\n",
    "    most_freq_pair = max(pairs, key=pairs.get)  # Get pair with max frequency\n",
    "    return most_freq_pair\n",
    "\n",
    "def bytePairEncoding(corpus:str, k: int):\n",
    "    \"\"\"\n",
    "    Function that returns a list of k merged pairs of characters from the input string.\n",
    "\n",
    "    @param corpus: The corpus\n",
    "    @param integer k: The number of merges\n",
    "\n",
    "    @return list: A list of k merged tokens (subwords)\n",
    "    \"\"\"\n",
    "\n",
    "    C = tokenizeCorpus(corpus)\n",
    "    # Initialize vocabulary with individual characters\n",
    "    vocab = set()\n",
    "    for word in C:\n",
    "        vocab.update(word.split())  # Add individual tokens (characters + '_') to vocab\n",
    "\n",
    "    # Merge tokens k times\n",
    "    for i in range(k):\n",
    "        # Step 1: Find the most frequent pair of adjacent tokens\n",
    "        tl, tr = MostFreqPair(C)\n",
    "\n",
    "        # Step 2: Create a new token by merging the pair\n",
    "        tn = ''.join(tl) + ''.join(tr)\n",
    "\n",
    "        # Step 3: Add the new token to the vocabulary\n",
    "        vocab.add(tn)\n",
    "\n",
    "        # Step 4: Update the corpus with the new token\n",
    "        updated_corpus = []\n",
    "        for word in C:\n",
    "            # Replace the pair with the new merged token\n",
    "            updated_word = word.replace(' '.join([tl, tr]), tn)\n",
    "            updated_corpus.append(updated_word)\n",
    "        \n",
    "        # Update the corpus for the next iteration\n",
    "        C = updated_corpus\n",
    "\n",
    "        # Print the result of the current iteration\n",
    "        prettyPrintMerge(C, i, list(vocab), [tl, tr])\n",
    "\n",
    "    # Return the final vocabulary after k merges\n",
    "    #return vocab\n",
    "\n",
    "def prettyPrintMerge(C: list[str], i: int, vocab: list[str], current:tuple[str], detailed:bool=False) -> None:\n",
    "    \"\"\"\n",
    "    A function to pretty-print each merge of the BPE algorithm.\n",
    "\n",
    "    @param list[str] C: Corpus\n",
    "    @param int k: The iteration number\n",
    "    @param list[str] vocab: The vocabulary at the current iteration\n",
    "    \"\"\"\n",
    "    if detailed:\n",
    "        print(f'After {i+1} merge(s):')\n",
    "\n",
    "    print(f'merge: \\t\\t{(current[0]), current[1]}')\n",
    "\n",
    "    if detailed:\n",
    "        print('Corpus:')\n",
    "        wordFrequency = Counter(C)\n",
    "        for word, freq in wordFrequency.items():\n",
    "            print(f'{freq} \\t {word}')\n",
    "        print('Vocabulary \\t'+', '.join(sorted(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducing example in chapter 2.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge: \t\t('e', 'r')\n",
      "merge: \t\t('er', '_')\n",
      "merge: \t\t('n', 'e')\n",
      "merge: \t\t('ne', 'w')\n",
      "merge: \t\t('l', 'o')\n",
      "merge: \t\t('lo', 'w')\n",
      "merge: \t\t('new', 'er_')\n",
      "merge: \t\t('low', '_')\n"
     ]
    }
   ],
   "source": [
    "corpus = 'low low low low low lowest lowest newer newer newer newer newer newer wider wider wider new new'\n",
    "k = 8\n",
    "vocab = bytePairEncoding(corpus, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordPiece\n",
    "\n",
    "Implement *WordPiece* from scratch using basic python functionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://huggingface.co/learn/nlp-course/en/chapter6/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeCorpusWP(corpus:str):\n",
    "    words = corpus.split()\n",
    "\n",
    "    tokenizedCorpus= []\n",
    "    for word in words:\n",
    "        tokenizedWord = [word[0]]\n",
    "        for letter in word[1:]:\n",
    "            tokenizedWord.append(f'##{letter}')\n",
    "        tokenizedCorpus.append(tokenizedWord)\n",
    "\n",
    "    return tokenizedCorpus\n",
    "\n",
    "\n",
    "\n",
    "def createVocab(tokenizedCorpus:str)-> set:\n",
    "    \"\"\"\n",
    "    Function that returns a dictionary with token frequenciesfor WP \n",
    "    @param corpus: Corpus in string format\n",
    "    @return: Dictionary with word frequencies\n",
    "    \"\"\" \n",
    "\n",
    "    #tokenizedCorpus = tokenizeCorpusWP(corpus)\n",
    "\n",
    "    tokenFreq = {}\n",
    "\n",
    "    for tokens in tokenizedCorpus:\n",
    "        for token in tokens:\n",
    "            if token in tokenFreq:\n",
    "                tokenFreq[token] += 1\n",
    "            else:\n",
    "                tokenFreq[token] = 1\n",
    "\n",
    "    return tokenFreq\n",
    "\n",
    "\n",
    "def getPairs(tokenizedCorpus:str) -> dict:\n",
    "    \"\"\"\n",
    "    A simple function that splits the corpus into pairs\n",
    "    \"\"\"\n",
    "    #tokenizedCorpus = tokenizeCorpusWP(corpus)\n",
    "\n",
    "\n",
    "    pairs = {}\n",
    "    for tokens in tokenizedCorpus:\n",
    "        for i in range(len(tokens)-1):\n",
    "            pair = (tokens[i], tokens[i+1])\n",
    "\n",
    "            if pair in pairs:\n",
    "                pairs[pair] += 1\n",
    "            else: pairs[pair] = 1\n",
    "\n",
    "    return pairs\n",
    "    \n",
    "\n",
    "\n",
    "def computePairScore(pair, pairCount, tokenFreq)-> float:\n",
    "    t1, t2 = pair\n",
    "\n",
    "    # get the frequency of each token\n",
    "    t1freq = tokenFreq.get(t1, 0)\n",
    "    t2freq = tokenFreq.get(t2, 0)\n",
    "\n",
    "    if t1freq == 0 or t2freq == 0:\n",
    "        return 0\n",
    "        \n",
    "    \n",
    "    return pairCount / (t1freq * t2freq)\n",
    "\n",
    "\n",
    "def computePairsScores(corpus:str) -> dict:\n",
    "\n",
    "    pairCounts = getPairs(corpus)\n",
    "    tokenFreq = createVocab(corpus)\n",
    "\n",
    "    pairScore = {}\n",
    "\n",
    "    for pair, pairCount in pairCounts.items():\n",
    "        score = computePairScore(pair, pairCount, tokenFreq)\n",
    "        pairScore[pair] = score\n",
    "\n",
    "    return pairScore\n",
    "\n",
    "\n",
    "def mergePair(pair, tokenizedCorpus, pairScores, tokenFreq):\n",
    "    newToken = pair[0] + pair[1][2:]\n",
    "\n",
    "    updatedCorpus = []\n",
    "\n",
    "    for word in tokenizedCorpus:\n",
    "        i = 0\n",
    "        while i < len(word)-1:\n",
    "            if (word[i], word[i+1]) == pair:\n",
    "                word[i] = newToken\n",
    "                del word[i+1]\n",
    "            else:\n",
    "                i += 1\n",
    "        updatedCorpus.append(word)\n",
    "    \n",
    "    # Recompute token frequencies and pair scores for the updated corpus\n",
    "    tokenFreq = createVocab(updatedCorpus)  # Recompute token frequencies\n",
    "    pairScores = computePairsScores(updatedCorpus)  # Recompute pair scores\n",
    "    \n",
    "    return updatedCorpus, tokenFreq, pairScores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def wordPiece(corpus:str, numMerges:int=10):\n",
    "\n",
    "    tokenizedCorpus = tokenizeCorpusWP(corpus)\n",
    "    tokenFreq = createVocab(tokenizedCorpus)\n",
    "    pairScores = computePairsScores(tokenizedCorpus)\n",
    "    \n",
    "    for i in range(numMerges):\n",
    "        if not pairScores:\n",
    "            print('no more pairs to merge')\n",
    "            break\n",
    "\n",
    "        highestScore = max(pairScores, key=pairScores.get)\n",
    "        print(f'merging pair: {highestScore} with score {pairScores[highestScore]}')\n",
    "\n",
    "        corpus, tokenFreq, pairScores = mergePair(highestScore, tokenizedCorpus, pairScores, tokenFreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging pair: ('##s', '##t') with score 0.5\n",
      "merging pair: ('w', '##i') with score 0.3333333333333333\n",
      "merging pair: ('wi', '##d') with score 0.3333333333333333\n",
      "merging pair: ('l', '##o') with score 0.14285714285714285\n",
      "merging pair: ('lo', '##w') with score 0.06666666666666667\n",
      "{'low': 7, '##e': 19, '##st': 2, 'n': 8, '##w': 8, '##r': 9, 'wid': 3}\n"
     ]
    }
   ],
   "source": [
    "wordPiece(corpus, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing BPE with WordPiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge: \t\t('e', 'r')\n",
      "merge: \t\t('er', '_')\n",
      "merge: \t\t('n', 'e')\n",
      "merge: \t\t('ne', 'w')\n",
      "merge: \t\t('l', 'o')\n"
     ]
    }
   ],
   "source": [
    "bytePairEncoding(corpus, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging pair: ('##s', '##t') with score 0.5\n",
      "merging pair: ('w', '##i') with score 0.3333333333333333\n",
      "merging pair: ('wi', '##d') with score 0.3333333333333333\n",
      "merging pair: ('l', '##o') with score 0.14285714285714285\n",
      "merging pair: ('lo', '##w') with score 0.06666666666666667\n"
     ]
    }
   ],
   "source": [
    "wordPiece(corpus, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a different corpus to compare the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = 'lorem ipsum dolor si amet'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
